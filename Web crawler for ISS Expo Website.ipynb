{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Crawler for ISS Expo info\n",
    "\n",
    "This notebook crawls the ISS Expo website and scrapes all company names, booth numbers, company website names and related e-mail addresses. It uses a commercial API to prevent blocking and the standard tools BeautifulSoup, requests and regex for Web Scraping. Pandas is used to manage the data in dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports packages\n",
    "from bs4 import BeautifulSoup # for Scraping\n",
    "import requests # for getting the website data\n",
    "import time\n",
    "import pandas as pd # for handling data in dataframes\n",
    "import re # for regex operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the functions that search the HTML soup for items\n",
    "\n",
    "def get_company_name(soup):\n",
    "    ####\n",
    "    ### Scrapes company name from mainpage\n",
    "    ###\n",
    "    content_company_name = []\n",
    "    for link in soup.find_all(\"a\", {\"class\":\"exhibitorName\"}):\n",
    "        text = link.get_text()\n",
    "        text = text.strip()\n",
    "        content_company_name.append(text)\n",
    "    return content_company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link(soup):\n",
    "    ####\n",
    "    ### Scrapes company URL from mainpage\n",
    "    ###\n",
    "    links = []\n",
    "    for link in soup.find_all(\"a\", {\"class\":\"exhibitorName\"}):\n",
    "            text = link.get(\"href\")\n",
    "            text = text.strip()\n",
    "            links.append(text)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_booth(soup):\n",
    "    ####\n",
    "    ### Scrapes booth number from mainpage\n",
    "    ###\n",
    "    content_booth = []\n",
    "    for link in soup.find_all(\"a\", {\"class\":\"boothLabel aa-mapIt\"}):\n",
    "        text = link.get_text()\n",
    "        text = text.strip()\n",
    "        content_booth.append(text)\n",
    "    return content_booth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_website(soup):\n",
    "    ####\n",
    "    ### Scrapes external company URL from each individual company page\n",
    "    ###\n",
    "    if soup.find_all(\"a\", {\"class\":\"aa-BoothContactUrl\"}):\n",
    "        for link in soup.find_all(\"a\", {\"class\":\"aa-BoothContactUrl\"}):\n",
    "            text = link.get_text()\n",
    "            text = text.strip()\n",
    "    else:\n",
    "        text = \"No Website\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the login data, URLs, APIs\n",
    "f = open(\"token.txt\", \"r\") # read token from local textfile for security reasons\n",
    "access_token = f.read()\n",
    "f.close()\n",
    "\n",
    "api_url = 'http://api.scraperapi.com' # using external scraping API to circumvent blocking and captchas\n",
    "url = \"https://iss.a2zinc.net/LongBeach2020/Public/exhibitors.aspx?ID=20972\" # URL to scrape\n",
    "payload = {'api_key': access_token, 'url': url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the entire HTML code from the mainpage\n",
    "r = requests.get(api_url, params=payload) \n",
    "soup = BeautifulSoup(r.content, 'html.parser') \n",
    "#print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check: length of all items (names, links, booth numers), should be equal count\n",
    "print(len(get_company_name(soup)))\n",
    "print(len(get_link(soup)))\n",
    "print(len(get_booth(soup)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Dataframe and storing the data\n",
    "df = pd.DataFrame() # creating empty DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the Company names\n",
    "df[\"Company_Name\"] = get_company_name(soup) # Scrapes company names and adds them into the DF as \"Company_Name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the Company's booth number\n",
    "df[\"Booth\"] = get_booth(soup) # Scrapes booth numbers and adds them into the DF as \"Booth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the Company websites\n",
    "df[\"Internal_link\"] = get_link(soup) # Scrapes internal links and adds them into the DF as \"Internal_link\"\n",
    "df[\"Link\"] = \"https://iss.a2zinc.net/LongBeach2020/Public/\" + df[\"Internal_link\"] # Adds full URL from scraped data \n",
    "df = df.drop(\"Internal_link\", axis=1) # Columns is no longer needed and gets deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a list of internal company links that get scraped to find the actual company websites\n",
    "internal_links = df[\"Link\"].tolist()\n",
    "\n",
    "websites = []\n",
    "\n",
    "for link in internal_links: # Scraping each individual link in internal_links and stores website URLs in a list\n",
    "    url = link\n",
    "    payload = {'api_key': access_token, 'url': url}\n",
    "    time.sleep(1)\n",
    "    r = requests.get(api_url, params=payload)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    websites.append(get_website(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check; lenghts of websites should be equal to the ones of names, links, booth numbers\n",
    "len(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Website\"] = websites # Adds scraped website names into the DF as \"Website\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find email addresses on individual company websites\n",
    "urls = df.iloc[:, 3] #getting urls from column 3\n",
    "\n",
    "email_list = [] # Creates an empty list for storing the scraped e-mail addresses\n",
    "\n",
    "# This loop is going through all the URLs and searching for e-mail addresses on those URLs, if none is found some generic contact page urls that might or might not exist are checked\n",
    "for website in urls:\n",
    "    for url in [\"\", \"/contact/\", \"/contact-us/\", \"/helpcenter/\", \"/contact/contact-us/\"]:\n",
    "        page = str(website) + str(url)\n",
    "        print(\"Parsing \"+ page)\n",
    "        try:\n",
    "            payload = {'api_key': access_token, 'url': page}\n",
    "            time.sleep(1)\n",
    "            r = requests.get(api_url, params=payload)\n",
    "            soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "            emails = re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,4}\",str(soup)) # This regex expression is the pattern to find e-mail addresses in a text\n",
    "            email_list.append(emails[0])\n",
    "            print(\"Mail: \", emails[0])\n",
    "        except:\n",
    "            email_list.append(\"No E-Mail\")\n",
    "            print(\"No Mail\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the emails that we scraped\n",
    "email_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting rid of unwanted technical support email addresses\n",
    "email_list = [i.replace('support@scraperapi.com',\"No E-Mail\") for i in email_list]\n",
    "email_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cleaned list of e-mail addresses is stored in the DF in the column \"E-Mail\"\n",
    "df[\"E-Mail\"] = email_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally saving data as a csv file\n",
    "df.to_csv(\"ISS expo info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reopen csv to create a new dataframe again with first column as index, just in case the work gets interrupted\n",
    "df = pd.read_csv(\"ISS expo info.csv\", index_col=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
