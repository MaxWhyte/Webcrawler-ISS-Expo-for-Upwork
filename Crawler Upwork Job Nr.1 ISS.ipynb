{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get company name from mainpage\n",
    "def get_company_name(soup):\n",
    "    content_company_name = []\n",
    "    for link in soup.find_all(\"a\", {\"class\":\"exhibitorName\"}):\n",
    "        text = link.get_text()\n",
    "        text = text.strip()\n",
    "        content_company_name.append(text)\n",
    "    return content_company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all internal links from mainpage\n",
    "def get_link(soup):\n",
    "    links = []\n",
    "    for link in soup.find_all(\"a\", {\"class\":\"exhibitorName\"}):\n",
    "            text = link.get(\"href\")\n",
    "            text = text.strip()\n",
    "            links.append(text)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get booth number from mainpage\n",
    "def get_booth(soup):\n",
    "    content_booth = []\n",
    "    for link in soup.find_all(\"a\", {\"class\":\"boothLabel aa-mapIt\"}):\n",
    "        text = link.get_text()\n",
    "        text = text.strip()\n",
    "        content_booth.append(text)\n",
    "    return content_booth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get external url from each individual company page\n",
    "def get_website(soup):\n",
    "    if soup.find_all(\"a\", {\"class\":\"aa-BoothContactUrl\"}):\n",
    "        for link in soup.find_all(\"a\", {\"class\":\"aa-BoothContactUrl\"}):\n",
    "            text = link.get_text()\n",
    "            text = text.strip()\n",
    "    else:\n",
    "        text = \"No Website\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_website(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://iss.a2zinc.net/LongBeach2020/Public/exhibitors.aspx?ID=20972\" # url to scrape\n",
    "\n",
    "f = open(\"token.txt\", \"r\") # read token from local textfile for security reasons\n",
    "access_token = f.read()\n",
    "f.close()\n",
    "\n",
    "payload = {'api_key': access_token, 'url': #using external API to circumvent blocking and captchas\n",
    "url}\n",
    "r = requests.get('http://api.scraperapi.com', params=payload) #scraping the mainpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking length of all items, should be equal count\n",
    "print(len(get_company_name(soup)))\n",
    "print(len(get_link(soup)))\n",
    "print(len(get_booth(soup)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame() # creating empty DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Company_Name\"] = get_company_name(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Internal_link\"] = get_link(soup)\n",
    "df[\"Link\"] = \"https://iss.a2zinc.net/LongBeach2020/Public/\" + df[\"Internal_link\"]\n",
    "df = df.drop(\"Internal_link\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Booth\"] = get_booth(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_links = df[\"Link\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = []\n",
    "\n",
    "for link in internal_links: # scraping each individual link in internal links list\n",
    "    url = link\n",
    "    payload = {'api_key': access_token, 'url': url}\n",
    "    time.sleep(1)\n",
    "    r = requests.get('http://api.scraperapi.com', params=payload)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    websites.append(get_website(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Website\"] = websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#safe as csv\n",
    "df.to_csv(\"ISS expo info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reopen csv to create DF again with first column as index\n",
    "df = pd.read_csv(\"ISS expo info.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test scraping with last x urls\n",
    "df_sample = df.iloc[-1:]\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find email addresses on individual websites\n",
    "urls = df_sample.iloc[:, 3] #getting urls from column 3\n",
    "email_list = []\n",
    "\n",
    "for url in urls:\n",
    "    print(\"Parsing \", url)\n",
    "    try:\n",
    "        payload = {'api_key': access_token, 'url': url}\n",
    "        time.sleep(1)\n",
    "        r = requests.get('http://api.scraperapi.com', params=payload)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    \n",
    "        emails = re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,4}\",str(soup))\n",
    "        email_list.append(emails[0])\n",
    "        print(\"Mail: \", emails[0])\n",
    "        \n",
    "    except:\n",
    "        try:\n",
    "            payload = {'api_key': access_token, 'url': url + \"/contact/\"}\n",
    "            time.sleep(1)\n",
    "            r = requests.get('http://api.scraperapi.com', params=payload)\n",
    "            soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "            emails = re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,4}\",str(soup))\n",
    "            email_list.append(emails[0])\n",
    "            print(\"Mail: \", emails[0])\n",
    "        except:\n",
    "            try:\n",
    "                payload = {'api_key': access_token, 'url': url + \"/helpcenter/\"}\n",
    "                time.sleep(1)\n",
    "                r = requests.get('http://api.scraperapi.com', params=payload)\n",
    "                soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "                emails = re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,4}\",str(soup))\n",
    "                email_list.append(emails[0])\n",
    "                print(\"Mail: \", emails[0])\n",
    "            except:\n",
    "                try:\n",
    "                    payload = {'api_key': access_token, 'url': url + \"/contact/contact-us/\"}\n",
    "                    time.sleep(1)\n",
    "                    r = requests.get('http://api.scraperapi.com', params=payload)\n",
    "                    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "                    emails = re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,4}\",str(soup))\n",
    "                    email_list.append(emails[0])\n",
    "                    print(\"Mail: \", emails[0])\n",
    "                except:\n",
    "                    try:\n",
    "                        payload = {'api_key': access_token, 'url': url + \"/contact/\"}\n",
    "                        time.sleep(1)\n",
    "                        r = requests.get('http://api.scraperapi.com', params=payload)\n",
    "                        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "                        emails = re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,4}\",str(soup))\n",
    "                        email_list.append(emails[0])\n",
    "                        print(\"Mail: \", emails[0])\n",
    "                    except:\n",
    "                        try:\n",
    "                            payload = {'api_key': access_token, 'url': url + \"/contact-us/\"}\n",
    "                            time.sleep(1)\n",
    "                            r = requests.get('http://api.scraperapi.com', params=payload)\n",
    "                            soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "                            emails = re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,4}\",str(soup))\n",
    "                            email_list.append(emails[0])\n",
    "                        except:\n",
    "                            try:\n",
    "                                payload = {'api_key': access_token, 'url': url + \"/contact-us\"}\n",
    "                                time.sleep(1)\n",
    "                                r = requests.get('http://api.scraperapi.com', params=payload)\n",
    "                                soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "                                emails = re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,4}\",str(soup))\n",
    "                                email_list.append(emails[0])\n",
    "                            except:\n",
    "                                email_list.append(\"No E-Mail\")\n",
    "                                print(\"No Mail\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the emails that we scraped\n",
    "email_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting rid of incorrect support... emails\n",
    "email_list = [i.replace('support@scraperapi.com',\"No E-Mail\") for i in email_list]\n",
    "email_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: \n",
    "#-check all websites when enough api calls are available again\n",
    "#-add email column to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
